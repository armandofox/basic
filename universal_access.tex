
\section{UNIVERSAL ACCESS AND COMPUTATIONAL THINKING}

%% \makequotation{Thinking like a computer scientist means more than being able to 
%% program a computer. It requires thinking at multiple levels of
%% abstraction.}{Jeannette M. Wing, \emph{Computational
%%     Thinking}~\cite{wing_computational_thinking}} 

\makequotation{I think everyone should learn how to program a computer,
  because it teaches you how to think.}{Steve Jobs~\cite{steve_jobs_interview}}


For a while in the 80s and 90s, ``computer literacy'' meant familiarity
with productivity tools such as email, Web browsers, word processors,
spreadsheets, and so on.
But within the computer science community, discussions about computer
literacy have always been framed quite differently.
In 1996, MIT computer scientist and educator \w{Seymour Papert} coined
the term \w{computational thinking} to describe the mindset and types of
techniques that characterize a computer scientist's view of
problem-solving.
Ten years later, an
\href{http://www.cs.cmu.edu/afs/cs/usr/wing/www/publications/Wing06.pdf}{influential
op-ed} by Carnegie-Mellon University computer scientist Jeannette Wing
argued for the practical importance of computational thinking and
problem solving~\cite{wing_computational_thinking}.
In 2013, two influential computer scientists, remarking on the demand
for computational thinking in virtually every career path \emph{outside}
of computer science,
stated simply
``Computing is the new
literacy.''~\cite{ieee_computer_special_issue_computing_education}.
The ``digital divide'' is no longer just about citizens lacking 
access to computers, but about citizens lacking any understanding of how
they perform 
their tasks and how they augment human intellect.  As Rushkoff says, in
this environment we must either ``program or be
programmed''~\cite{rushkoff}.

\begin{tangent}
In the 2006 Salon article~\cite{why_johnny_cant_code},
science fiction author David Brin laments that for all its flaws, and
despite its small view of the world (or perhaps because of it), BASIC
was sufficiently nonthreatening to introduce an entire generation of
newbies to the joy of programming---exactly the goals of its
creators---whereas today's more expressive languages for higher-powered
platforms may scare beginners away.

Many angry responses to Brin's article pointed out that BASIC
instills habits contrary to modern OO programming practices (true),
that powerful scripting languages like Perl and Python have filled BASIC's
niche (true), and so on.  These responses miss the point.  
Perl and Python (for example) are languages
targeted at professional programmers, not beginners, and they
reflect two decades of thinking and experience focused on a professional audience,
not a student audience.  These languages can be intimidatingly powerful
for beginners (though Python can be suitably simplified by simply
not using some of its advanced features).
A site developed in response to Brin's article, 
\href{http://quitebasic.com}{Quite BASIC}, provides a simple in-browser
BASIC environment with medium-resolution color graphics, and is probably
the effort closest in spirit to what Brin praises.
\end{tangent}

In 1962, Tom Kurtz, a mathematics professor at Dartmouth concerned
about all of these same things, approached
his department chair John Kemeny with a remarkably farsighted vision:
Since computers were clearly going to be important in everyday
life, \emph{all} Dartmouth students should become computer literate.  This idea
was
particularly progressive for a liberal arts college
where 75\% of the students were nontechnical majors~\cite{goto},
given that the lack of mass-produced software in 1962 implied that
``computer literacy'' meant learning to write simple programs.
Furthermore, Kurtz wanted students to have computer access as easily as
they could borrow library books---just type their student ID number into
a computer and they could start working.  This was a radical proposal at
a time when computers were so expensive that they were rented by the
month rather than purchased, with a typical monthly rent far
exceeding a professor's monthly salary, so that
most universities charged
students for actual 
computer time used,  incentivizing them to minimize actual
usage. 

Like many premier universities, Dartmouth could mitigate the cost of
acquiring computers for education by cultivating good relationships with
computer manufacturers and raising funds from donors and government
grants.
But Kurtz would also need to overcome a technology problem.
Programmers in 1962 would prepare programs on decks of punched cards,
submit them to a technician,
% TBD picture of punched card
and come back hours or days later to pick up a printout with
the results of their program run.
(It may seem ridiculous today to wait hours or days for the results of a
running a computer program, but given the speeds of 
communication and transportation in 1962, it was normal for
computer-generated business information to be a few days out of
date~\cite{ceruzzi}.) 
If the results were botched because of a simple error in the program,
the programmer would re-punch the offending cards, resubmit the job, and
again wait hours or days for a result.

This method of working, called \w{batch processing}, guaranteed the
computer would never be idle as long as there was a long queue of user
jobs waiting.
Batch processing favored efficient use of computer time over
programmer convenience, an understandable
tradeoff given the high cost of computers.
But Kurtz feared that liberal arts students with no
extrinsic career motivation to learn about computers would be unwilling
to endure the ritual of
punching cards and waiting days for an answer.
Fortunately, a technical milestone saved the day.
Professor John McCarthy, then at MIT, had been part of a group working
on an experimental technology called
\w{timesharing}~\cite{corbato62timesharing}, in which a computer
switches its attention many times per second among jobs corresponding to
different users.
Because the computer can switch its attention much more quickly than
users can read or type, each user has the illusion that the computer
is waiting only on him or her.

Timesharing would turn out to be a revolutionary technological innovation,
but it wasn't taken seriously in industry at that time since batch
processing provided a more efficient use of the expensive computer's
time.  
However, McCarthy easily persuaded Kurtz and Kemeny that such interactive
computer use would be perfect for education because it would allow
students to get
immediate feedback, eliminating the annoying ``rituals'' of batch
processing that might discourage beginners.

But timesharing would increase costs, since
many students using the computer at the same time meant that each student
would need their own terminal for typing in programs.
Forunately, while video terminals were still a decade away, 
%% and popular printing terminals
%% \href{http://www.science.uva.nl/museum/ibm1050.php}{IBM 1050}
%% cost \$??? each, 
%% and the
%% were slow and expensive---the
%% Friden Flexowriter printed only 10 characters per second and
%% \href{http://retrotechnology.com/herbs_stuff/flex_behr.html}{cost
%%   \$2500-4000}, or nearly \$20,000 in 2014 dollars.
the Teletype Corporation had recently designed a rugged and
inexpensive ``teleprinter'' for the US~Navy, the Model~ASR-33 (Automatic
Send/Receive terminal), and had made it
available to the retail market in 1963 at a price of only \$700 (\$5,200
in 2014)---about a quarter of the cost of existing printing
terminals---and sold over
half a million of them before discontinuing the product in 1981.

Of the various
computer vendors, only General Electric's computers could handle the
multiple simultaneous terminals that would enable
timesharing. 
But timesharing was still an experimental technology, so the Dartmouth
team would have to create their own
timesharing operating system based on the MIT prototype.  
Dartmouth's timesharing system was ultimately so successful that it
catapulted GE into a new timesharing business that was profitable for a
few years, and led GE to later offer Dartmouth a brand-new model computer in
exchange for collaboration on re-creating timesharing and BASIC on it.

BASIC's birth directly reflects the rapid shift from batch processing to
timesharing: 
BASIC was the first programming language to include a command that made
the program pause and wait for the user to type
something, a concept that made no sense in batch processing and was
absent from the high level languages such as FORTRAN and COBOL that had
been developed up to that time.
 



% TBD photo of Teletype 33, Flexowriter, IBM 1050


